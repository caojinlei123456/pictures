[toc]



# python

## PE8

```
1.变量
全局变量：USER_CONSATNT
私有变量:_private_value 只用于警告说明，外部类仍然可以去访问
内置变量:__class__

2.函数(蛇形命名)
私有方法:def _secrete(self):与私有变量类似，只是警告说明其为内部函数，但外部类任然可以访问
特殊方法：def __add__(self): 很少使用
函数参数:def connect(self):
			self._user = user

3.类（驼峰命名）
```



## python问题

### 1.生成器的使用？？

```
主要用于内存使用不足，感觉有点像一个滑动窗口，他不是保留一个容器，而是保留形成这个容器的算法，然后利用循环迭代逐个生成。
生成方式：
	1.generator=(x for x in range(10))
	2.yield关键词
备注：生成器与普通函数的区别在于其不是顺序执行的，而是在yield处返回，在调用next执行
next和send的区别在于，send传递的参数会作为yield表达式的值，而yield的参数时返回给调用者的值，也就是说send可以强行修改上一个yield表达式值。
```

### 2.装饰器到底是啥，能都手写一个装饰器？？

```
作用：最简单的解释就是不改变原先函数或类的内容，给他增加功能。
类型：1. 函数装饰器
        a.不带参数，两层函数
        c.带参数，三层函数

    2. 类装饰器
        必须包含__init__,__call__.
        a.不带参数，init写的func，
        b.带参数，init写的参数，call写的fun，call内部包含函数
from functools import wraps
@wraps(func)  # 将被修改的函数的一些属性值赋值给修饰器函数，最终让属性的显示更符合我们的感官
```

### 3.python的内存管理机构?

```
当python对象的引用计数降为0时，没有任何引用指向该对象，该对象就可以被垃圾回收，不过有个阈值，具体的可以看
import gc
gc.get_threshold()
(700,10,10)
700即是垃圾回收启动的阈值，每10次0代垃圾回收，会配合1次1代回收；每10次1代的垃圾回收，才会有1次的2代垃圾回收。
```

### 4.容器是什么

```
包括字典，列表，元组，集合。我的理解就是字面意思，包含所有的一个容纳器物。
```

### 5. 赋值、浅拷贝和深拷贝有什么区别

```
独立程度赋值<浅拷贝<深拷贝（完全独立）
赋值：引用的是相同的对象，所以只要原始对象发送修改，赋值变量也跟着修改。

浅拷贝：a和b均为独立变量，但他们的子对象还是引用同一对象。

深拷贝：完全拷贝了父对象和子对象，两者完全独立

什么是子对象？个人理解，比如a=[1,2,3,4,[3,4,5]],[3,4,5]中的3,4,5就是子对象，因此当3,4,5发送改变时，浅拷贝也会变化。
```

### 6. 偏函数是什么，作用是什么

```
参考partial_learn库
偏函数：重新定义一个函数，只是固定了原函数的部分参数
作用：当需要重复实验时，固定相关的函数
```

### 7.read/readline/readlines/的区别

```
1.read读取所有文件
2.readline([size])包含最大字节数，只读取1行，不是迭代对象
3.readlines感觉是个迭代器，可以循环读取数据
```

### 8.json模块dumps/loads/dump/load的使用

```
dumps:将dict、list、tuple转化为str
dump:用于将dict保存成json文件
loads：将str转化为dict、list(tuple也转化成list)
load:用于将json装化为dict
```

### 9.pandas中map、apply、applymap、agg、transfomer的用法

```
1.apply通用，即可用于series也可以用于DataFrame。
2.applymap用于所有元素。
3.agg在groupby以后使用，用法比较多，以后有机会慢慢熟练。

#详细看pandas_learn
```

### 11.@property的作用和使用场景

```
作用：针对外部访问内部属性，提供get和set方法是经常会用到的功能，前面讲过，装饰器是在不改变函数的前提下添加功能。
使用场景：如果单纯使用@property，外部初步传入数字后，就没法更改，相当于只提供了get功能，但不提供set。如果要set需要添加@xxx.setter，从外部传参修改（主要是可以通过这个限制传参阈值）

参考wrappers
```

### 12.可迭代对象的操作（add，append,pop,remove,insert）

```
add只用于集合。
append用于列表之间的拼接。
pop在list和set的用法不同，前者可以指定索引，不指定默认pop最晚进入list的元素，后者默认pop最早进入set的元素。
remove在list和set的用法相同，删除指定内容元素。
insert与pop相反，在list中指定索引进行添加

参考iterable_action
```

### 13.callback（回调函数的作用）

```
就是以前说函数里面传函数的用法，这个函数就叫做回调函数
参考CallBackDemo
```

### 14.python find

```

```

### 15.Counter

```

```

### 16.macro、micro、weight的区别

```
micro：就是多分类中的accuracy
macro：认为每个样本权重一样
weight：认为每个样本权重不一样
比如：
t=[0,0,0,1,1,1,2,2]
p=[0,0,0,0,1,2,1,1]
macro_p = (3/4+1/3+0)/3 =13/36
weight_p = (3/8)*(3/4)+(3/8)*(1/3)+(2/8)*0 =13/32
micro_p = 4/8=0.5
```

### 17.eval

```python
将str内的list、dic、tuple提取出来
demo查看eval_learn
```





# linux

### 查看程序详细信息

```
ll /proc/PID(具体的PID)
```

### 查看运行进程

```
ps aux | grep python # 查看具有python正在运行的进程
```

### 复制并移动

```python
cp -r /123.txt /data2/cjl/  # 递归复制
scp -r /123.txt root@10.35.163.3:/home/caojinlei
```

### >/>>/</&的作用

```
> bash 123.sh >my.txt    # 将123.sh的输出结果覆盖到my.txt
>> bash 123.sh >>my.txt  # 将123,sh的输出结果追加到my.txt
<   					 # 和>反向的意思，用的不多
&  						 # 最后追加，将结果在后台运行。
bash 123.sh >my.txt 2>&1 # 将123.sh的输出结果覆盖到my.txt 且有错误结果也存在my.txt中，2表示错误输出，1表示标准输出，就是前面的my.txt
```

### 误差操作

1. lsof | grep p_person_feature # 寻找pic

```bash
sftp-serv 380700          root    3r      REG               0,99 15840683463  474670088 /root/p_person_feature.txt (deleted)
```

2. cd /proc/380700/fd
3. ll

```
lr-x------ 1 root root 64 Jan  7 11:23 0 -> pipe:[3281483245]
l-wx------ 1 root root 64 Jan  7 11:23 1 -> pipe:[3281483246]
l-wx------ 1 root root 64 Jan  7 11:23 2 -> pipe:[3281483247]
lr-x------ 1 root root 64 Jan  7 11:23 3 -> /root/p_person_feature.txt (deleted)
```

4. cp 3 ~/p_person_feature.txt

### 压缩和解压缩命令

**tar**

```
解包：tar zxvf filename.tar
打包：tar czvf filename.tar dirname
```

# Spark

## Spark问题

### 1.Spark的计算在内存里执行的，难道python不行吗？python不也是用内存计算，

```
spark的特点就是数据交互不走磁盘，多个任务的数据交互不走硬盘，但是spark的shuffle过程和哈斗殴片一样走磁盘

Spark的DAG（又向无环图），这个DAG就相当于改进版的Mapreduce，他可以说是由多个MapReduce组成，当数据处理存在多个map和reduce，Mapreduce需要提交多个job，而spark只提交一次。
MapReduce每次shuffle操作后，必须写到磁盘，而如果数据需要反复用的，spark会cache到内存中，方便迭代时使用。
```

### 2.job、stage、task的联系和区别

```
Job：action的触发会生成一个job，Job会提交给DAG，分解成Stage，

Stage：每一个job有一个或者多个Stage，一次shuffle切分成两个Stage，一次transfomer就会形成一词shuffle。

Task：一个stage有多个task，task由分区决定，有多少分区就有多少task，task数量是我们最大的并行度

core：executors中有多少个cpu就有多少core，如果task超过core，就先执行core个，剩下的task等待。
```

### 3.spark调优

1. 开发调优
   1. 尽可能只使用一个RDD，对多次使用的RDD进行持久化，持久化存在多种策略可以利用.persist()进行手动设置。
   2. 
2. 资源调优
3. 数据倾斜
4. shuffle调优

### 4.shuffle过程

### 





# HIVE

## 1.hive udf（python脚本+Hive前缀）

### python脚本

```python
import sys
for line in sys.stdin:
    arg = line.strip().split('\t')  # 输入项进行切分
    jd = arg[0]
    wd = arg[1]
    kkbh = arg[2]
    try:
        geohsh = encode(float(wd), float(jd))  
        print('\t'.join([kkbh, geohsh]))  #输出项用print
    except Exception as e:
        print(e)
```

### hive前缀

```sql
add file @HUA_HIVE_RESOURCE(to_geohash.py); # 增加udf资源
select 
      transform(dqjd,dqwd,kkdwbh) using '/opt/dm_python3/bin/python to_geohash.py'
      as (kkdwbh,geohash)
from
    (
        select * from ods_gj_kkgc_di 
    ) p1
limit 100
```

## 2.with as 用法

```sql
with c as (select jhdf from p) 将结果C作为临时结果表存在内存中
```



# 算法

## 1.NLP

###  1.1Word2vec和Fasttext的区别和联系是什么？

## 2.ML


### 2.1.KNN的底层算法和原理，CDP的底层算法和原理，如何优化？

### 2.2半监督学习的方法。

### 2.3.如何构建KNN图（ANN图中的细节）

## 2.4.H20机器学习平台



# 系统和架构

## 1.分布式和集群的区别

“分头做事和一堆人的区别”

**分布式**：像spark那样，某个集群也可以作为分布式一个节点。

**集群**：它是一种物理形态，多台单节点支撑相同的服务，

## 2.微服务是什么

微服务强调系统按业务边界做细粒度的拆分和部署，比如将整体工程拆分为：支付业务，订单业务，用户业务……每个业务又包括各自独立的前端、后端、数据库人员和相应的测试人员。